{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calling Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 store twitter credential into Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import json\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Get tweets from Twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API credentials\n",
    "\n",
    "with open('twitter_credentials.json') as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "    consumer_key = info['CONSUMER_KEY']\n",
    "    consumer_secret = info['CONSUMER_SECRET']\n",
    "    access_key = info['ACCESS_TOKEN']\n",
    "    access_secret = info['ACCESS_SECRET']\n",
    "    \n",
    "# Create the api endpoint\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "myfile = open(\"twitter_data_kill.txt\", 'w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class listener(StreamListener):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        all_data = json.loads(data)\n",
    "        \n",
    "        tweet = all_data[\"text\"]\n",
    "        place_countryCode = 'None'\n",
    "        place_fullName = 'None'\n",
    "        place_coordinates = 'None'\n",
    "        place = all_data['place']\n",
    "        if all_data['place'] != None:\n",
    "            place_countryCode = all_data['place']['country_code']\n",
    "            place_fullName = all_data['place']['full_name']\n",
    "            place_coordinates = json.dumps(all_data['place']['bounding_box']['coordinates'])    \n",
    "        if 'extended_tweet' in all_data:\n",
    "            tweet = all_data['extended_tweet']['full_text']\n",
    "        username = all_data[\"user\"][\"screen_name\"]\n",
    "        user_location = all_data[\"user\"][\"location\"]\n",
    "        id = all_data[\"user\"][\"id\"]\n",
    "        geo_enabled = all_data[\"user\"][\"geo_enabled\"]\n",
    "        tweet = tweet.replace('\\n', ' ').replace('\\r', '')\n",
    "        myfile.write(str(id) + '|' + str(username) + '|' + str(user_location) + '|' + str(tweet) + '|' + str(geo_enabled) + '|' + str(place_countryCode) + '|' + str(place_fullName) + '|' + str(place_coordinates) + \"\\n\")        \n",
    "        print((id,username,user_location,tweet,geo_enabled,place_countryCode, place_fullName, place_coordinates))\n",
    "        self.count = self.count + 1\n",
    "        print(self.count)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.set_access_token(access_key, access_secret)\n",
    "twitterStream = Stream(auth, listener())\n",
    "LOCATIONS = [-124.7771694, 24.520833, -66.947028, 49.384472,        # Contiguous US\n",
    "                 -164.639405, 58.806859, -144.152365, 71.76871,         # Alaska\n",
    "                 -160.161542, 18.776344, -154.641396, 22.878623]        # Hawaii\n",
    "twitterStream.filter(track=['suicidal', 'suicide', 'kill myself', 'my suicide note'\n",
    "                            'my suicide', 'letter', 'end my life', \"never wake up\", \"can't go on\", 'not worth living'\n",
    "                            'ready to jump', 'sleep forever', 'want to die', 'be dead', 'better off without me', \n",
    "                            'better off dead', 'suicide plan', 'suicide pact', \"tired of living\", \"don't want to be here\", \n",
    "                            'die alone', \"go to sleep forever\"], locations=LOCATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Import twitter data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from textblob import Word \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "test = pd.read_csv(\"/Users/yeezhianliew/Desktop/twitter_data_us_new_clean.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User_id</th>\n",
       "      <th>Username</th>\n",
       "      <th>User_Location</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Geo_Enabled</th>\n",
       "      <th>Place_CountryCode</th>\n",
       "      <th>Place_FullName</th>\n",
       "      <th>Place_Coordinates</th>\n",
       "      <th>centroid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>Text</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>P_POS</th>\n",
       "      <th>P_NEG</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>412770174</td>\n",
       "      <td>momariedavison</td>\n",
       "      <td>Minnetonka, MN</td>\n",
       "      <td>truly sooo glad i’ve been distancing myself fr...</td>\n",
       "      <td>True</td>\n",
       "      <td>US</td>\n",
       "      <td>Prior Lake, MN</td>\n",
       "      <td>[[[-93.489043, 44.685054], [-93.489043, 44.761...</td>\n",
       "      <td>(44.723144500000004, -93.434323)</td>\n",
       "      <td>44.723144</td>\n",
       "      <td>-93.434323</td>\n",
       "      <td>truly sooo glad i’ve been distancing myself fr...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>74008370</td>\n",
       "      <td>juarus</td>\n",
       "      <td>Gadsden, Alabama</td>\n",
       "      <td>little smoked chicken potpie action eastsidepi...</td>\n",
       "      <td>True</td>\n",
       "      <td>US</td>\n",
       "      <td>Gadsden, AL</td>\n",
       "      <td>[[[-86.079043, 33.933091], [-86.079043, 34.073...</td>\n",
       "      <td>(34.003125, -86.00682950000001)</td>\n",
       "      <td>34.003125</td>\n",
       "      <td>-86.006830</td>\n",
       "      <td>A little smoked chicken potpie action#eastside...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>844713218</td>\n",
       "      <td>Tessv234</td>\n",
       "      <td>San Marcos, Texas</td>\n",
       "      <td>u had to show hem the lizzie mcguire episode o...</td>\n",
       "      <td>True</td>\n",
       "      <td>US</td>\n",
       "      <td>San Marcos, TX</td>\n",
       "      <td>[[[-97.993537, 29.840708], [-97.993537, 29.950...</td>\n",
       "      <td>(29.8956065, -97.9351915)</td>\n",
       "      <td>29.895606</td>\n",
       "      <td>-97.935192</td>\n",
       "      <td>@fleetw0odsnacc U had to show hem the Lizzie M...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>321221241</td>\n",
       "      <td>jpenntaughtme</td>\n",
       "      <td>No Name, CO</td>\n",
       "      <td>clown  now we know everything you say is just ...</td>\n",
       "      <td>True</td>\n",
       "      <td>US</td>\n",
       "      <td>Colorado, USA</td>\n",
       "      <td>[[[-109.060257, 36.992427], [-109.060257, 41.0...</td>\n",
       "      <td>(38.997935999999996, -105.5508905)</td>\n",
       "      <td>38.997936</td>\n",
       "      <td>-105.550891</td>\n",
       "      <td>You been a clown, now we know everything you s...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>926663250036514816</td>\n",
       "      <td>rosegirl14hotm2</td>\n",
       "      <td>United States</td>\n",
       "      <td>love it   here comes the tide rolling</td>\n",
       "      <td>True</td>\n",
       "      <td>US</td>\n",
       "      <td>Alabama, USA</td>\n",
       "      <td>[[[-88.473228, 30.144425], [-88.473228, 35.008...</td>\n",
       "      <td>(32.576227, -86.68073749999999)</td>\n",
       "      <td>32.576227</td>\n",
       "      <td>-86.680737</td>\n",
       "      <td>@AlabamaDieHards I love it,  here comes the Ti...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             User_id         Username      User_Location  \\\n",
       "0           0           412770174   momariedavison     Minnetonka, MN   \n",
       "1           1            74008370           juarus   Gadsden, Alabama   \n",
       "2           2           844713218         Tessv234  San Marcos, Texas   \n",
       "3           3           321221241    jpenntaughtme        No Name, CO   \n",
       "4           4  926663250036514816  rosegirl14hotm2      United States   \n",
       "\n",
       "                                               Tweet  Geo_Enabled  \\\n",
       "0  truly sooo glad i’ve been distancing myself fr...         True   \n",
       "1  little smoked chicken potpie action eastsidepi...         True   \n",
       "2  u had to show hem the lizzie mcguire episode o...         True   \n",
       "3  clown  now we know everything you say is just ...         True   \n",
       "4             love it   here comes the tide rolling          True   \n",
       "\n",
       "  Place_CountryCode  Place_FullName  \\\n",
       "0                US  Prior Lake, MN   \n",
       "1                US     Gadsden, AL   \n",
       "2                US  San Marcos, TX   \n",
       "3                US   Colorado, USA   \n",
       "4                US    Alabama, USA   \n",
       "\n",
       "                                   Place_Coordinates  \\\n",
       "0  [[[-93.489043, 44.685054], [-93.489043, 44.761...   \n",
       "1  [[[-86.079043, 33.933091], [-86.079043, 34.073...   \n",
       "2  [[[-97.993537, 29.840708], [-97.993537, 29.950...   \n",
       "3  [[[-109.060257, 36.992427], [-109.060257, 41.0...   \n",
       "4  [[[-88.473228, 30.144425], [-88.473228, 35.008...   \n",
       "\n",
       "                             centroid   latitude   longitude  \\\n",
       "0    (44.723144500000004, -93.434323)  44.723144  -93.434323   \n",
       "1     (34.003125, -86.00682950000001)  34.003125  -86.006830   \n",
       "2           (29.8956065, -97.9351915)  29.895606  -97.935192   \n",
       "3  (38.997935999999996, -105.5508905)  38.997936 -105.550891   \n",
       "4     (32.576227, -86.68073749999999)  32.576227  -86.680737   \n",
       "\n",
       "                                                Text  Retweet  P_POS  P_NEG  \\\n",
       "0  truly sooo glad i’ve been distancing myself fr...    False  0.766   0.00   \n",
       "1  A little smoked chicken potpie action#eastside...    False  0.000   0.00   \n",
       "2  @fleetw0odsnacc U had to show hem the Lizzie M...    False  0.218   0.00   \n",
       "3  You been a clown, now we know everything you s...    False  0.000   0.38   \n",
       "4  @AlabamaDieHards I love it,  here comes the Ti...    False  0.824   0.00   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          0  \n",
       "2          1  \n",
       "3         -1  \n",
       "4          1  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Tweet']=test['Tweet'].fillna(\"\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/yeezhianliew/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User_id</th>\n",
       "      <th>Username</th>\n",
       "      <th>User_Location</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Geo_Enabled</th>\n",
       "      <th>Place_CountryCode</th>\n",
       "      <th>Place_FullName</th>\n",
       "      <th>Place_Coordinates</th>\n",
       "      <th>centroid</th>\n",
       "      <th>...</th>\n",
       "      <th>P_NEG</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lower_case</th>\n",
       "      <th>Special_word</th>\n",
       "      <th>Contents</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>short_word</th>\n",
       "      <th>string</th>\n",
       "      <th>NonEnglish</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>412770174</td>\n",
       "      <td>momariedavison</td>\n",
       "      <td>Minnetonka, MN</td>\n",
       "      <td>truly sooo glad i’ve been distancing myself fr...</td>\n",
       "      <td>True</td>\n",
       "      <td>US</td>\n",
       "      <td>Prior Lake, MN</td>\n",
       "      <td>[[[-93.489043, 44.685054], [-93.489043, 44.761...</td>\n",
       "      <td>(44.723144500000004, -93.434323)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>truly sooo glad i’ve been distancing myself fr...</td>\n",
       "      <td>[truly, sooo, glad, i, ve, been, distancing, m...</td>\n",
       "      <td>truly sooo glad i’ve been distancing myself fr...</td>\n",
       "      <td>['truly', 'sooo', 'glad', 'distancing', 'speci...</td>\n",
       "      <td>[truly, sooo, glad, distancing, specific, peop...</td>\n",
       "      <td>truly sooo glad distancing specific people fee...</td>\n",
       "      <td>truly glad specific people feel becoming happi...</td>\n",
       "      <td>truly glad specific people feel becoming happi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>74008370</td>\n",
       "      <td>juarus</td>\n",
       "      <td>Gadsden, Alabama</td>\n",
       "      <td>little smoked chicken potpie action eastsidepi...</td>\n",
       "      <td>True</td>\n",
       "      <td>US</td>\n",
       "      <td>Gadsden, AL</td>\n",
       "      <td>[[[-86.079043, 33.933091], [-86.079043, 34.073...</td>\n",
       "      <td>(34.003125, -86.00682950000001)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>little smoked chicken potpie action eastsidepi...</td>\n",
       "      <td>[little, smoked, chicken, potpie, action, east...</td>\n",
       "      <td>little smoked chicken potpie action eastsidepi...</td>\n",
       "      <td>['little', 'smoked', 'chicken', 'potpie', 'act...</td>\n",
       "      <td>[little, smoked, chicken, potpie, action, east...</td>\n",
       "      <td>little smoked chicken potpie action eastsidepi...</td>\n",
       "      <td>little smoked chicken potpie action</td>\n",
       "      <td>little smoked chicken potpie action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>844713218</td>\n",
       "      <td>Tessv234</td>\n",
       "      <td>San Marcos, Texas</td>\n",
       "      <td>u had to show hem the lizzie mcguire episode o...</td>\n",
       "      <td>True</td>\n",
       "      <td>US</td>\n",
       "      <td>San Marcos, TX</td>\n",
       "      <td>[[[-97.993537, 29.840708], [-97.993537, 29.950...</td>\n",
       "      <td>(29.8956065, -97.9351915)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>u had to show hem the lizzie mcguire episode o...</td>\n",
       "      <td>[u, had, to, show, hem, the, lizzie, mcguire, ...</td>\n",
       "      <td>u had to show hem the lizzie mcguire episode o...</td>\n",
       "      <td>['u', 'show', 'hem', 'lizzie', 'mcguire', 'epi...</td>\n",
       "      <td>[show, hem, lizzie, mcguire, episode, singing,...</td>\n",
       "      <td>show hem lizzie mcguire episode singing christ...</td>\n",
       "      <td>show hem episode singing special suite life</td>\n",
       "      <td>show hem episode singing special suite life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>321221241</td>\n",
       "      <td>jpenntaughtme</td>\n",
       "      <td>No Name, CO</td>\n",
       "      <td>clown  now we know everything you say is just ...</td>\n",
       "      <td>True</td>\n",
       "      <td>US</td>\n",
       "      <td>Colorado, USA</td>\n",
       "      <td>[[[-109.060257, 36.992427], [-109.060257, 41.0...</td>\n",
       "      <td>(38.997935999999996, -105.5508905)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-1</td>\n",
       "      <td>clown  now we know everything you say is just ...</td>\n",
       "      <td>[clown, now, we, know, everything, you, say, i...</td>\n",
       "      <td>clown now we know everything you say is just s...</td>\n",
       "      <td>['clown', 'know', 'everything', 'say', 'dumb',...</td>\n",
       "      <td>[clown, know, everything, say, dumb, shit, get...</td>\n",
       "      <td>clown know everything say dumb shit get people...</td>\n",
       "      <td>clown know everything say dumb get people talk...</td>\n",
       "      <td>clown know everything say dumb get people talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>926663250036514816</td>\n",
       "      <td>rosegirl14hotm2</td>\n",
       "      <td>United States</td>\n",
       "      <td>love it   here comes the tide rolling</td>\n",
       "      <td>True</td>\n",
       "      <td>US</td>\n",
       "      <td>Alabama, USA</td>\n",
       "      <td>[[[-88.473228, 30.144425], [-88.473228, 35.008...</td>\n",
       "      <td>(32.576227, -86.68073749999999)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>love it   here comes the tide rolling</td>\n",
       "      <td>[love, it, here, comes, the, tide, rolling]</td>\n",
       "      <td>love it here comes the tide rolling</td>\n",
       "      <td>['love', 'comes', 'tide', 'rolling']</td>\n",
       "      <td>[love, comes, tide, rolling]</td>\n",
       "      <td>love comes tide rolling</td>\n",
       "      <td>love comes tide rolling</td>\n",
       "      <td>love come tide rolling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             User_id         Username      User_Location  \\\n",
       "0           0           412770174   momariedavison     Minnetonka, MN   \n",
       "1           1            74008370           juarus   Gadsden, Alabama   \n",
       "2           2           844713218         Tessv234  San Marcos, Texas   \n",
       "3           3           321221241    jpenntaughtme        No Name, CO   \n",
       "4           4  926663250036514816  rosegirl14hotm2      United States   \n",
       "\n",
       "                                               Tweet  Geo_Enabled  \\\n",
       "0  truly sooo glad i’ve been distancing myself fr...         True   \n",
       "1  little smoked chicken potpie action eastsidepi...         True   \n",
       "2  u had to show hem the lizzie mcguire episode o...         True   \n",
       "3  clown  now we know everything you say is just ...         True   \n",
       "4             love it   here comes the tide rolling          True   \n",
       "\n",
       "  Place_CountryCode  Place_FullName  \\\n",
       "0                US  Prior Lake, MN   \n",
       "1                US     Gadsden, AL   \n",
       "2                US  San Marcos, TX   \n",
       "3                US   Colorado, USA   \n",
       "4                US    Alabama, USA   \n",
       "\n",
       "                                   Place_Coordinates  \\\n",
       "0  [[[-93.489043, 44.685054], [-93.489043, 44.761...   \n",
       "1  [[[-86.079043, 33.933091], [-86.079043, 34.073...   \n",
       "2  [[[-97.993537, 29.840708], [-97.993537, 29.950...   \n",
       "3  [[[-109.060257, 36.992427], [-109.060257, 41.0...   \n",
       "4  [[[-88.473228, 30.144425], [-88.473228, 35.008...   \n",
       "\n",
       "                             centroid  \\\n",
       "0    (44.723144500000004, -93.434323)   \n",
       "1     (34.003125, -86.00682950000001)   \n",
       "2           (29.8956065, -97.9351915)   \n",
       "3  (38.997935999999996, -105.5508905)   \n",
       "4     (32.576227, -86.68073749999999)   \n",
       "\n",
       "                         ...                          P_NEG  sentiment  \\\n",
       "0                        ...                           0.00          1   \n",
       "1                        ...                           0.00          0   \n",
       "2                        ...                           0.00          1   \n",
       "3                        ...                           0.38         -1   \n",
       "4                        ...                           0.00          1   \n",
       "\n",
       "                                          lower_case  \\\n",
       "0  truly sooo glad i’ve been distancing myself fr...   \n",
       "1  little smoked chicken potpie action eastsidepi...   \n",
       "2  u had to show hem the lizzie mcguire episode o...   \n",
       "3  clown  now we know everything you say is just ...   \n",
       "4             love it   here comes the tide rolling    \n",
       "\n",
       "                                        Special_word  \\\n",
       "0  [truly, sooo, glad, i, ve, been, distancing, m...   \n",
       "1  [little, smoked, chicken, potpie, action, east...   \n",
       "2  [u, had, to, show, hem, the, lizzie, mcguire, ...   \n",
       "3  [clown, now, we, know, everything, you, say, i...   \n",
       "4        [love, it, here, comes, the, tide, rolling]   \n",
       "\n",
       "                                            Contents  \\\n",
       "0  truly sooo glad i’ve been distancing myself fr...   \n",
       "1  little smoked chicken potpie action eastsidepi...   \n",
       "2  u had to show hem the lizzie mcguire episode o...   \n",
       "3  clown now we know everything you say is just s...   \n",
       "4                love it here comes the tide rolling   \n",
       "\n",
       "                                          stop_words  \\\n",
       "0  ['truly', 'sooo', 'glad', 'distancing', 'speci...   \n",
       "1  ['little', 'smoked', 'chicken', 'potpie', 'act...   \n",
       "2  ['u', 'show', 'hem', 'lizzie', 'mcguire', 'epi...   \n",
       "3  ['clown', 'know', 'everything', 'say', 'dumb',...   \n",
       "4               ['love', 'comes', 'tide', 'rolling']   \n",
       "\n",
       "                                          short_word  \\\n",
       "0  [truly, sooo, glad, distancing, specific, peop...   \n",
       "1  [little, smoked, chicken, potpie, action, east...   \n",
       "2  [show, hem, lizzie, mcguire, episode, singing,...   \n",
       "3  [clown, know, everything, say, dumb, shit, get...   \n",
       "4                       [love, comes, tide, rolling]   \n",
       "\n",
       "                                              string  \\\n",
       "0  truly sooo glad distancing specific people fee...   \n",
       "1  little smoked chicken potpie action eastsidepi...   \n",
       "2  show hem lizzie mcguire episode singing christ...   \n",
       "3  clown know everything say dumb shit get people...   \n",
       "4                            love comes tide rolling   \n",
       "\n",
       "                                          NonEnglish  \\\n",
       "0  truly glad specific people feel becoming happi...   \n",
       "1                little smoked chicken potpie action   \n",
       "2        show hem episode singing special suite life   \n",
       "3  clown know everything say dumb get people talk...   \n",
       "4                            love comes tide rolling   \n",
       "\n",
       "                                               tweet  \n",
       "0  truly glad specific people feel becoming happi...  \n",
       "1                little smoked chicken potpie action  \n",
       "2        show hem episode singing special suite life  \n",
       "3  clown know everything say dumb get people talk...  \n",
       "4                             love come tide rolling  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import Word \n",
    "test['lower_case']= test['Tweet'].apply(lambda x: x.lower())       #convert upper to lower case\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "test['Special_word'] = test.apply(lambda row: tokenizer.tokenize(row['lower_case']), axis=1)     #tokenize word\n",
    "\n",
    "freq = pd.Series(' '.join(test['Tweet']).split()).value_counts()[-10:]                       \n",
    "freq = list(freq.index)\n",
    "test['Contents'] = test['Tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))  #remove less frequent words\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "test['stop_words'] = test['Special_word'].apply(lambda x: [item for item in x if item not in stop])   #remove stop word\n",
    "\n",
    "test['stop_words'] = test['stop_words'].astype('str')\n",
    "test['short_word'] = test['stop_words'].str.findall('\\w{3,}')            #remove words less than 3 characters\n",
    "test['string'] = test['stop_words'].replace({\"'\": '', ',': ''}, regex=True)\n",
    "test['string'] = test['string'].str.findall('\\w{3,}').str.join(' ') \n",
    "\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "test['NonEnglish'] = test['string'].apply(lambda x: \" \".join(x for x in x.split() if x in words))  #remove non english word\n",
    "\n",
    "test['tweet'] = test['NonEnglish'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) # convert it into root words\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_most_common_words =5000\n",
    "max_len = 30\n",
    "tokenizer = Tokenizer(num_words=n_most_common_words)\n",
    "tokenizer.fit_on_texts(test['tweet'].values)\n",
    "seq = tokenizer.texts_to_sequences(test['tweet'].values)\n",
    "padded = pad_sequences(seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Load Sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('/Users/yeezhianliew/Desktop/model1.json', 'r')                  \n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)                        #load the model\n",
    "loaded_model.load_weights(\"/Users/yeezhianliew/Desktop/model1.h1\")       #load weights into new model\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.6 Predict Sentiment on Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28025565 0.7197443 ]\n",
      " [0.31371388 0.68628615]\n",
      " [0.79426694 0.20573306]\n",
      " ...\n",
      " [0.45268574 0.54731417]\n",
      " [0.45268577 0.5473142 ]\n",
      " [0.45268577 0.5473142 ]]\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(padded)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.280256</td>\n",
       "      <td>0.719744</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.313714</td>\n",
       "      <td>0.686286</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.794267</td>\n",
       "      <td>0.205733</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.293821</td>\n",
       "      <td>0.706179</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.424152</td>\n",
       "      <td>0.575848</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Positive  Negative Sentiment\n",
       "0  0.280256  0.719744  Negative\n",
       "1  0.313714  0.686286  Negative\n",
       "2  0.794267  0.205733  Positive\n",
       "3  0.293821  0.706179  Negative\n",
       "4  0.424152  0.575848  Negative"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing1 = pd.DataFrame(result)\n",
    "testing1.columns = [\"Positive\", \"Negative\"]\n",
    "testing1['Sentiment'] = testing1.idxmax(axis=1)\n",
    "testing1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_most_common_words =1000\n",
    "max_len = 20\n",
    "tokenizer = Tokenizer(num_words=n_most_common_words)\n",
    "tokenizer.fit_on_texts(test['tweet'].values)\n",
    "seq = tokenizer.texts_to_sequences(test['tweet'].values)\n",
    "padded = pad_sequences(seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Load Suicide Ideation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('/Users/yeezhianliew/Desktop/model_w.json', 'r')                  \n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_models = model_from_json(loaded_model_json)                      #load the model\n",
    "loaded_models.load_weights(\"/Users/yeezhianliew/Desktop/model_w.h5\")    #load weights into new model\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.8 Predict Suicide Ideation on Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03479315 0.96520686]\n",
      " [0.02105558 0.9789444 ]\n",
      " [0.04934056 0.9506594 ]\n",
      " ...\n",
      " [0.01289478 0.98710525]\n",
      " [0.01289478 0.98710525]\n",
      " [0.02849706 0.9715029 ]]\n"
     ]
    }
   ],
   "source": [
    "results = loaded_models.predict(padded)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Potential_Suicide_Post</th>\n",
       "      <th>Not_Suicide_Post</th>\n",
       "      <th>Suicide_Ideation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034793</td>\n",
       "      <td>0.965207</td>\n",
       "      <td>Not_Suicide_Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.978944</td>\n",
       "      <td>Not_Suicide_Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.950659</td>\n",
       "      <td>Not_Suicide_Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.095111</td>\n",
       "      <td>0.904889</td>\n",
       "      <td>Not_Suicide_Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.982317</td>\n",
       "      <td>Not_Suicide_Post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Potential_Suicide_Post  Not_Suicide_Post  Suicide_Ideation\n",
       "0                0.034793          0.965207  Not_Suicide_Post\n",
       "1                0.021056          0.978944  Not_Suicide_Post\n",
       "2                0.049341          0.950659  Not_Suicide_Post\n",
       "3                0.095111          0.904889  Not_Suicide_Post\n",
       "4                0.017683          0.982317  Not_Suicide_Post"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing2 = pd.DataFrame(results)                                          # Create a Data frame for result\n",
    "testing2.columns = [\"Potential_Suicide_Post\", \"Not_Suicide_Post\"]\n",
    "testing2['Suicide_Ideation'] = testing2.idxmax(axis=1)\n",
    "testing2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame({'User_id':test['User_id'],'Username':test['Username'],\n",
    "                             'User_location':test['User_Location'],'Place_CountryCode' :test['Place_CountryCode'],\n",
    "                             'Place_FullName':test['Place_FullName'],'Original_Tweet':test['Tweet'],'Clean_Tweet':test['tweet'],  \n",
    "                             'Place_Coordinates':test['Place_Coordinates'],'Sentiment':testing1['Sentiment'],\n",
    "                             'Suicide_Ideation':testing2['Suicide_Ideation']})    #create the full result csv file\n",
    "final_result.to_csv('/Users/yeezhianliew/Desktop/output.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
