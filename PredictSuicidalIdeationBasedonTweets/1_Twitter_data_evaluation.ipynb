{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Suicidal Ideation Based on Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>Suicidal ideation detection in online social networks is an emerging research area with major challenges. Recent research has shown that the publicly available information, spread across social media platforms, holds valuable indicators for effectively detecting individuals with suicidal intentions. The key challenge of suicide prevention is understanding and detecting the complex risk factors and warning signs that may precipitate the event. We present an approach that uses the social media platform <b>Twitter</b> to quantify suicide warning signs for individuals and to detect posts containing suicide-related content. The main originality of this approach is the automatic identification of sudden changes in a user's online behavior. To detect such changes, we combine natural language processing(NLP) techniques to aggregate behavioral and textual features and pass these features through a model framework, which is widely used for change detection in data.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color='DodgerBlue'>This notebook, classifier the Tweets as 'Positive'/'Negative'. This is done by using the following techniques :\n",
    "    <ul>\n",
    "        <li>Import the data</li>\n",
    "        <li>Data Cleaning - Removing Null, Missing Values, Renaming Columns</li>\n",
    "        <li>Data Preprocessing - Lower-casing, NLTK, Removing Stop Words, Language Filtering, Lemmetization</li>\n",
    "        <li>Count Vectorizer</li>\n",
    "        <li>Modeling - Gaussian NB, Bernoulli NB, Random Forest, Ensemble, Decision Tree, Gradient Boosting, XGradient Boosting, AdaBoost. Deep Learning - 1-layer LSTM, 2-Layer LSTM, CNN + 2-LSTM</li>\n",
    "        <li>K-Fold Cross Validation</li>\n",
    "    </ul>\n",
    "</font>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Sentiment Train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import tree\n",
    "from textblob import Word \n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from keras.layers import Conv1D ,MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer \n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Flatten\n",
    "%matplotlib inline\n",
    "\n",
    "Dataset = pd.read_csv(\"/Users/yeezhianliew/Desktop/Tweets.csv\",encoding =\"ISO-8859-1\") \n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Twitter_Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674999</th>\n",
       "      <td>0</td>\n",
       "      <td>@leprcn I saw that! Yeah, did you get the emai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675000</th>\n",
       "      <td>0</td>\n",
       "      <td>Hanging with Rosy tonight and tomorrow. P.E. s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675001</th>\n",
       "      <td>0</td>\n",
       "      <td>@BrooksLazar Yay!!! You've conformed to a soci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675002</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm hungry.  I'll eat noodles and nothing but ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment                                      Twitter_Tweet\n",
       "0               0  is upset that he can't update his Facebook by ...\n",
       "674999          0  @leprcn I saw that! Yeah, did you get the emai...\n",
       "675000          0  Hanging with Rosy tonight and tomorrow. P.E. s...\n",
       "675001          0  @BrooksLazar Yay!!! You've conformed to a soci...\n",
       "675002          0  I'm hungry.  I'll eat noodles and nothing but ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.rename(columns={\n",
    "                 \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\": \"Twitter_Tweet\",\n",
    "                \"0\":'Sentiment',\n",
    "                \"_TheSpecialOne_\":\"Username\"},\n",
    "               inplace = True )\n",
    "Dataset = Dataset.drop(['1467810369','Mon Apr 06 22:19:45 PDT 2009','NO_QUERY','Username'], axis=1)  # 0:neg, 4: pos\n",
    "\n",
    "Dataset.head()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>\n",
    "    <ul><li>Lower-casing</li>\n",
    "    <li>NLTK</li> \n",
    "    <li>Removing Stop Words</li>\n",
    "    <li>Language Filtering</li>\n",
    "        <li>Lemmetization</li></ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/yeezhianliew/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Twitter_Tweet</th>\n",
       "      <th>lower_case</th>\n",
       "      <th>Special_word</th>\n",
       "      <th>Contents</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>short_word</th>\n",
       "      <th>string</th>\n",
       "      <th>NonEnglish</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>[is, upset, that, he, can, t, update, his, fac...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>['upset', 'update', 'facebook', 'texting', 'mi...</td>\n",
       "      <td>[upset, update, facebook, texting, might, cry,...</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "      <td>upset update might cry result school today als...</td>\n",
       "      <td>upset update might cry result school today als...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674999</th>\n",
       "      <td>0</td>\n",
       "      <td>@leprcn I saw that! Yeah, did you get the emai...</td>\n",
       "      <td>@leprcn i saw that! yeah, did you get the emai...</td>\n",
       "      <td>[leprcn, i, saw, that, yeah, did, you, get, th...</td>\n",
       "      <td>@leprcn I saw that! Yeah, did you get the emai...</td>\n",
       "      <td>['leprcn', 'saw', 'yeah', 'get', 'email', 'sen...</td>\n",
       "      <td>[leprcn, saw, yeah, get, email, sent, couple, ...</td>\n",
       "      <td>leprcn saw yeah get email sent couple months a...</td>\n",
       "      <td>saw yeah get sent couple ago going junk mail</td>\n",
       "      <td>saw yeah get sent couple ago going junk mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675000</th>\n",
       "      <td>0</td>\n",
       "      <td>Hanging with Rosy tonight and tomorrow. P.E. s...</td>\n",
       "      <td>hanging with rosy tonight and tomorrow. p.e. s...</td>\n",
       "      <td>[hanging, with, rosy, tonight, and, tomorrow, ...</td>\n",
       "      <td>Hanging with Rosy tonight and tomorrow. P.E. s...</td>\n",
       "      <td>['hanging', 'rosy', 'tonight', 'tomorrow', 'p'...</td>\n",
       "      <td>[hanging, rosy, tonight, tomorrow, starts, mon...</td>\n",
       "      <td>hanging rosy tonight tomorrow starts monday</td>\n",
       "      <td>hanging rosy tonight tomorrow</td>\n",
       "      <td>hanging rosy tonight tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675001</th>\n",
       "      <td>0</td>\n",
       "      <td>@BrooksLazar Yay!!! You've conformed to a soci...</td>\n",
       "      <td>@brookslazar yay!!! you've conformed to a soci...</td>\n",
       "      <td>[brookslazar, yay, you, ve, conformed, to, a, ...</td>\n",
       "      <td>@BrooksLazar Yay!!! You've conformed to a soci...</td>\n",
       "      <td>['brookslazar', 'yay', 'conformed', 'social', ...</td>\n",
       "      <td>[brookslazar, yay, conformed, social, trend, m...</td>\n",
       "      <td>brookslazar yay conformed social trend miss se...</td>\n",
       "      <td>social trend miss seeing everyday</td>\n",
       "      <td>social trend miss seeing everyday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675002</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm hungry.  I'll eat noodles and nothing but ...</td>\n",
       "      <td>i'm hungry.  i'll eat noodles and nothing but ...</td>\n",
       "      <td>[i, m, hungry, i, ll, eat, noodles, and, nothi...</td>\n",
       "      <td>I'm hungry. I'll eat noodles and nothing but n...</td>\n",
       "      <td>['hungry', 'eat', 'noodles', 'nothing', 'noodl...</td>\n",
       "      <td>[hungry, eat, noodles, nothing, noodles, bad]</td>\n",
       "      <td>hungry eat noodles nothing noodles bad</td>\n",
       "      <td>hungry eat nothing bad</td>\n",
       "      <td>hungry eat nothing bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment                                      Twitter_Tweet  \\\n",
       "0               0  is upset that he can't update his Facebook by ...   \n",
       "674999          0  @leprcn I saw that! Yeah, did you get the emai...   \n",
       "675000          0  Hanging with Rosy tonight and tomorrow. P.E. s...   \n",
       "675001          0  @BrooksLazar Yay!!! You've conformed to a soci...   \n",
       "675002          0  I'm hungry.  I'll eat noodles and nothing but ...   \n",
       "\n",
       "                                               lower_case  \\\n",
       "0       is upset that he can't update his facebook by ...   \n",
       "674999  @leprcn i saw that! yeah, did you get the emai...   \n",
       "675000  hanging with rosy tonight and tomorrow. p.e. s...   \n",
       "675001  @brookslazar yay!!! you've conformed to a soci...   \n",
       "675002  i'm hungry.  i'll eat noodles and nothing but ...   \n",
       "\n",
       "                                             Special_word  \\\n",
       "0       [is, upset, that, he, can, t, update, his, fac...   \n",
       "674999  [leprcn, i, saw, that, yeah, did, you, get, th...   \n",
       "675000  [hanging, with, rosy, tonight, and, tomorrow, ...   \n",
       "675001  [brookslazar, yay, you, ve, conformed, to, a, ...   \n",
       "675002  [i, m, hungry, i, ll, eat, noodles, and, nothi...   \n",
       "\n",
       "                                                 Contents  \\\n",
       "0       is upset that he can't update his Facebook by ...   \n",
       "674999  @leprcn I saw that! Yeah, did you get the emai...   \n",
       "675000  Hanging with Rosy tonight and tomorrow. P.E. s...   \n",
       "675001  @BrooksLazar Yay!!! You've conformed to a soci...   \n",
       "675002  I'm hungry. I'll eat noodles and nothing but n...   \n",
       "\n",
       "                                               stop_words  \\\n",
       "0       ['upset', 'update', 'facebook', 'texting', 'mi...   \n",
       "674999  ['leprcn', 'saw', 'yeah', 'get', 'email', 'sen...   \n",
       "675000  ['hanging', 'rosy', 'tonight', 'tomorrow', 'p'...   \n",
       "675001  ['brookslazar', 'yay', 'conformed', 'social', ...   \n",
       "675002  ['hungry', 'eat', 'noodles', 'nothing', 'noodl...   \n",
       "\n",
       "                                               short_word  \\\n",
       "0       [upset, update, facebook, texting, might, cry,...   \n",
       "674999  [leprcn, saw, yeah, get, email, sent, couple, ...   \n",
       "675000  [hanging, rosy, tonight, tomorrow, starts, mon...   \n",
       "675001  [brookslazar, yay, conformed, social, trend, m...   \n",
       "675002      [hungry, eat, noodles, nothing, noodles, bad]   \n",
       "\n",
       "                                                   string  \\\n",
       "0       upset update facebook texting might cry result...   \n",
       "674999  leprcn saw yeah get email sent couple months a...   \n",
       "675000        hanging rosy tonight tomorrow starts monday   \n",
       "675001  brookslazar yay conformed social trend miss se...   \n",
       "675002             hungry eat noodles nothing noodles bad   \n",
       "\n",
       "                                               NonEnglish  \\\n",
       "0       upset update might cry result school today als...   \n",
       "674999       saw yeah get sent couple ago going junk mail   \n",
       "675000                      hanging rosy tonight tomorrow   \n",
       "675001                  social trend miss seeing everyday   \n",
       "675002                             hungry eat nothing bad   \n",
       "\n",
       "                                                    tweet  \n",
       "0       upset update might cry result school today als...  \n",
       "674999       saw yeah get sent couple ago going junk mail  \n",
       "675000                      hanging rosy tonight tomorrow  \n",
       "675001                  social trend miss seeing everyday  \n",
       "675002                             hungry eat nothing bad  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset['lower_case']= Dataset['Twitter_Tweet'].apply(lambda x: x.lower())       #convert upper to lower case\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "Dataset['Special_word'] = Dataset.apply(lambda row: tokenizer.tokenize(row['lower_case']), axis=1)     #tokenize word\n",
    "\n",
    "freq = pd.Series(' '.join(Dataset['Twitter_Tweet']).split()).value_counts()[-10:]                       \n",
    "freq = list(freq.index)\n",
    "Dataset['Contents'] = Dataset['Twitter_Tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))  #remove less frequent words\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "Dataset['stop_words'] = Dataset['Special_word'].apply(lambda x: [item for item in x if item not in stop])   #remove stop word\n",
    "\n",
    "Dataset['stop_words'] = Dataset['stop_words'].astype('str')\n",
    "Dataset['short_word'] = Dataset['stop_words'].str.findall('\\w{3,}')            #remove words less than 3 characters\n",
    "Dataset['string'] = Dataset['stop_words'].replace({\"'\": '', ',': ''}, regex=True)\n",
    "Dataset['string'] = Dataset['string'].str.findall('\\w{3,}').str.join(' ') \n",
    "\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "Dataset['NonEnglish'] = Dataset['string'].apply(lambda x: \" \".join(x for x in x.split() if x in words))  #remove non english word\n",
    "\n",
    "Dataset['tweet'] = Dataset['NonEnglish'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) # convert it into root words\n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove Null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset['Sentiment'] =Dataset['Sentiment'].fillna(\"\")\n",
    "Dataset['tweet'] =Dataset['tweet'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(Dataset[\"tweet\"],Dataset[\"Sentiment\"], test_size = 0.33, random_state = 42)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Count Vectorizer + TFIDF Transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Using Count vectorizer combine with TFIDF transformer to convert raw document to tfidf matrix ,as words into binary number</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()                        #Convert a collection of text documents to a matrix of token counts\n",
    "transformer = TfidfTransformer(norm='l2',sublinear_tf=True) \n",
    "\n",
    "X_train_counts = count_vect.fit_transform(x_train)\n",
    "X_train = transformer.fit_transform(X_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(x_test)\n",
    "X_test= transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 train dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167500, 22) (167500,)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 test dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82500, 22) (82500,)\n"
     ]
    }
   ],
   "source": [
    "print (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'> Using various Machine learning classifiers to Train, Test and Predict and Validate them.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Using Decision Tree Classifier for Classification and generating the Classification Report.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Positive</th>\n",
       "      <th>True Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted Positive</th>\n",
       "      <td>23694</td>\n",
       "      <td>17379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Negative</th>\n",
       "      <td>16381</td>\n",
       "      <td>25046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    True Positive  True Negative\n",
       "Predicted Positive          23694          17379\n",
       "Predicted Negative          16381          25046"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = tree.DecisionTreeClassifier()\n",
    "model_1.fit(X_train,y_train)\n",
    "y_pred1 = model_1.predict(X_test)\n",
    "pd.DataFrame(                                                \n",
    "    confusion_matrix(y_test, y_pred1),                      #to check how good is your model prediction\n",
    "    columns=['True Positive', 'True Negative'],\n",
    "    index=['Predicted Positive', 'Predicted Negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.58      0.58     41073\n",
      "          4       0.59      0.60      0.60     41427\n",
      "\n",
      "avg / total       0.59      0.59      0.59     82500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred1)                                    #print the accuracy \n",
    "print(classification_report(y_test, y_pred1))                      #print the precision, recall and f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Validation for DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6871030303030303\n",
      "Cross-validated scores: [0.68182469 0.6816578  0.67895331]\n"
     ]
    }
   ],
   "source": [
    "scores_1 = cross_val_score(model_1, X_train,y_train, cv=3)   #3 fold validation\n",
    "print(accuracy_score(y_test,y_pred1))\n",
    "print (\"Cross-validated scores:\", scores_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Running the Random Forest with the following parameters and capturing the performance metrics\n",
    "    <ul><li>n-estimators = 50</li></ul></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6119272727272728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.64      0.62     41073\n",
      "          4       0.62      0.58      0.60     41427\n",
      "\n",
      "avg / total       0.61      0.61      0.61     82500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2 = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "model_2.fit(X_train,y_train)\n",
    "y_pred2 = model_2.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred2))\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Validation for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6119272727272728\n",
      "Cross-validated scores: [0.60987929 0.60720363 0.60840363]\n"
     ]
    }
   ],
   "source": [
    "scores_2 = cross_val_score(model_2, X_train,y_train, cv=3)   #3 fold validation\n",
    "print(accuracy_score(y_test,y_pred2))\n",
    "print (\"Cross-validated scores:\", scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Running the Random Forest with the following parameters and capturing the performance metrics\n",
    "    <ul><li>n-estimators =120</li></ul></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6167151515151515\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.64      0.63     41073\n",
      "          4       0.63      0.59      0.61     41427\n",
      "\n",
      "avg / total       0.62      0.62      0.62     82500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_3 = RandomForestClassifier(n_estimators=120, random_state=0)\n",
    "model_3.fit(X_train,y_train)\n",
    "y_pred3 = model_3.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred3))\n",
    "print(classification_report(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6167151515151515\n",
      "Cross-validated scores: [0.61575384 0.61134096 0.61048126]\n"
     ]
    }
   ],
   "source": [
    "scores_3 = cross_val_score(model_3, X_train,y_train, cv=3)   #3 fold validation\n",
    "print(accuracy_score(y_test,y_pred3))\n",
    "print (\"Cross-validated scores:\", scores_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Running the GaussianNB with the following parameters and capturing the performance metrics\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49773333333333336\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      1.00      0.66     41073\n",
      "          4       0.35      0.00      0.00     41427\n",
      "\n",
      "avg / total       0.43      0.50      0.33     82500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_4= GaussianNB()                                 #good at features have continuous values\n",
    "model_4.fit(X_train,y_train)\n",
    "y_pred4 = model_4.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred4))\n",
    "print(classification_report(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation for GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49773333333333336\n",
      "Cross-validated scores: [0.50112834 0.50097613 0.50108359]\n"
     ]
    }
   ],
   "source": [
    "scores_4 = cross_val_score(model_4, X_train,y_train, cv=3)   #3 fold validation\n",
    "print(accuracy_score(y_test,y_pred4))\n",
    "print (\"Cross-validated scores:\", scores_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Running the BernoulliNB with the following parameters and capturing the performance metrics\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5139030303030303\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.28      0.37     41073\n",
      "          4       0.51      0.74      0.61     41427\n",
      "\n",
      "avg / total       0.52      0.51      0.49     82500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5= BernoulliNB(fit_prior=True)        #word fequency less important, better result\n",
    "model_5.fit(X_train,y_train)\n",
    "y_pred5 = model_5.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(accuracy_score(y_test,y_pred5))\n",
    "print(classification_report(y_test, y_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation for BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5139030303030303\n",
      "Cross-validated scores: [0.51441774 0.51028961 0.51152544]\n"
     ]
    }
   ],
   "source": [
    "scores_5 = cross_val_score(model_5, X_train,y_train, cv=3)   #3 fold validation\n",
    "print(accuracy_score(y_test,y_pred5))\n",
    "print (\"Cross-validated scores:\", scores_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Running the GradientBoostingClassifier with the following parameters and capturing the performance metrics.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3855            2.31m\n",
      "         2           1.3834            2.29m\n",
      "         3           1.3827            3.35m\n",
      "         4           1.3817            3.71m\n",
      "         5           1.3800            3.31m\n",
      "         6           1.3792            3.17m\n",
      "         7           1.3788            2.99m\n",
      "         8           1.3783            2.84m\n",
      "         9           1.3777            2.78m\n",
      "        10           1.3770            2.74m\n",
      "        20           1.3688            2.40m\n",
      "        30           1.3660            2.23m\n",
      "        40           1.3628            2.25m\n",
      "        50           1.3574            2.21m\n",
      "        60           1.3545            2.09m\n",
      "        70           1.3501            1.98m\n",
      "        80           1.3465            1.96m\n",
      "        90           1.3428            1.83m\n",
      "       100           1.3377            1.72m\n",
      "       200           1.3164            1.21m\n",
      "       300           1.3027           38.10s\n",
      "       400           1.2918            0.00s\n",
      "0.6154909090909091\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.62      0.62     41073\n",
      "          4       0.62      0.61      0.61     41427\n",
      "\n",
      "avg / total       0.62      0.62      0.62     82500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6 = GradientBoostingClassifier(n_estimators=400,\n",
    "                                        max_features='auto', max_depth=2,          #log2 = 54% . auto= 55% ,sqrt=54\n",
    "                                        random_state=1, verbose=1)                  #auto, max_depth=2 - 63\n",
    "model_6.fit(X_train,y_train)                                                           #auto, max_depth=7 - 70\n",
    "y_pred6 = model_6.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred6))\n",
    "print(classification_report(y_test, y_pred6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation for Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3855           56.33s\n",
      "         2           1.3834            1.05m\n",
      "         3           1.3827            1.35m\n",
      "         4           1.3822            1.35m\n",
      "         5           1.3805            1.53m\n",
      "         6           1.3795            1.68m\n",
      "         7           1.3788            1.70m\n",
      "         8           1.3784            1.63m\n",
      "         9           1.3767            1.57m\n",
      "        10           1.3764            1.51m\n",
      "        20           1.3684            1.82m\n",
      "        30           1.3651            1.63m\n",
      "        40           1.3624            1.74m\n",
      "        50           1.3598            1.92m\n",
      "        60           1.3540            1.99m\n",
      "        70           1.3487            1.93m\n",
      "        80           1.3444            1.90m\n",
      "        90           1.3419            1.93m\n",
      "       100           1.3386            1.90m\n",
      "       200           1.3184           57.22s\n",
      "       300           1.3045           23.23s\n",
      "       400           1.2934            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3843            1.33m\n",
      "         2           1.3835            1.33m\n",
      "         3           1.3828            1.37m\n",
      "         4           1.3812            1.42m\n",
      "         5           1.3802            1.42m\n",
      "         6           1.3794            1.40m\n",
      "         7           1.3787            1.40m\n",
      "         8           1.3779            1.39m\n",
      "         9           1.3763            1.37m\n",
      "        10           1.3760            1.37m\n",
      "        20           1.3688            1.28m\n",
      "        30           1.3670            1.21m\n",
      "        40           1.3636            1.14m\n",
      "        50           1.3592            1.09m\n",
      "        60           1.3529            1.05m\n",
      "        70           1.3493            1.00m\n",
      "        80           1.3473           57.41s\n",
      "        90           1.3442           54.78s\n",
      "       100           1.3415           52.51s\n",
      "       200           1.3172           49.86s\n",
      "       300           1.3050           23.09s\n",
      "       400           1.2937            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3854           51.58s\n",
      "         2           1.3832            1.01m\n",
      "         3           1.3825            1.14m\n",
      "         4           1.3819            1.06m\n",
      "         5           1.3810            1.02m\n",
      "         6           1.3790           59.46s\n",
      "         7           1.3782           57.61s\n",
      "         8           1.3778           56.52s\n",
      "         9           1.3775           55.89s\n",
      "        10           1.3769           55.31s\n",
      "        20           1.3697           53.15s\n",
      "        30           1.3666           52.11s\n",
      "        40           1.3623           49.59s\n",
      "        50           1.3568           46.93s\n",
      "        60           1.3514           45.03s\n",
      "        70           1.3474           46.21s\n",
      "        80           1.3450           45.08s\n",
      "        90           1.3430           43.29s\n",
      "       100           1.3387           42.91s\n",
      "       200           1.3160           28.76s\n",
      "       300           1.3014           14.63s\n",
      "       400           1.2902            0.00s\n",
      "0.6154909090909091\n",
      "Cross-validated scores: [0.61548519 0.61642756 0.60727527]\n"
     ]
    }
   ],
   "source": [
    "scores_6 = cross_val_score(model_6, X_train,y_train, cv=3)   #3 fold validation\n",
    "print(accuracy_score(y_test,y_pred6))\n",
    "print (\"Cross-validated scores:\", scores_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Running the GradientBoostingClassifier with the following parameters and capturing the performance metrics.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3840            9.68m\n",
      "         2           1.3786           12.41m\n",
      "         3           1.3743           12.43m\n",
      "         4           1.3707           11.99m\n",
      "         5           1.3677           11.38m\n",
      "         6           1.3661           10.80m\n",
      "         7           1.3647           10.26m\n",
      "         8           1.3634           10.22m\n",
      "         9           1.3598           10.00m\n",
      "        10           1.3574            9.80m\n",
      "        20           1.3408            9.05m\n",
      "        30           1.3291            8.23m\n",
      "        40           1.3210            7.70m\n",
      "        50           1.3149            7.35m\n",
      "        60           1.3091            7.02m\n",
      "        70           1.3033            6.86m\n",
      "        80           1.2977            6.68m\n",
      "        90           1.2927            6.44m\n",
      "       100           1.2893            6.22m\n",
      "       200           1.2527            5.28m\n",
      "       300           1.2293            4.43m\n",
      "       400           1.2099            3.50m\n",
      "       500           1.1919            2.62m\n",
      "       600           1.1794            1.73m\n",
      "       700           1.1637           51.09s\n",
      "       800           1.1484            0.00s\n",
      "0.6770181818181819\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.66      0.67     41073\n",
      "          4       0.67      0.70      0.68     41427\n",
      "\n",
      "avg / total       0.68      0.68      0.68     82500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_7 = GradientBoostingClassifier(n_estimators=800,\n",
    "                                        max_features='auto', max_depth=4,           \n",
    "                                        random_state=1, verbose=1)\n",
    "model_7.fit(X_train, y_train)\n",
    "y_pred7 = model_7.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred7))\n",
    "print(classification_report(y_test, y_pred7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation of Grandient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3839            3.81m\n",
      "         2           1.3787            3.94m\n",
      "         3           1.3744            5.24m\n",
      "         4           1.3708            5.90m\n",
      "         5           1.3678            5.47m\n",
      "         6           1.3645            5.15m\n",
      "         7           1.3619            4.97m\n",
      "         8           1.3605            4.85m\n",
      "         9           1.3597            4.75m\n",
      "        10           1.3583            4.76m\n",
      "        20           1.3423            4.44m\n",
      "        30           1.3304            4.32m\n",
      "        40           1.3229            4.28m\n",
      "        50           1.3140            4.08m\n",
      "        60           1.3085            3.96m\n",
      "        70           1.3034            4.09m\n",
      "        80           1.2988            4.08m\n",
      "        90           1.2944            3.90m\n",
      "       100           1.2910            3.77m\n",
      "       200           1.2535            3.27m\n",
      "       300           1.2255            2.71m\n",
      "       400           1.2020            2.12m\n",
      "       500           1.1839            1.56m\n",
      "       600           1.1690            1.03m\n",
      "       700           1.1515           30.91s\n",
      "       800           1.1344            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3813           12.80m\n",
      "         2           1.3770           13.20m\n",
      "         3           1.3729           12.36m\n",
      "         4           1.3708           12.08m\n",
      "         5           1.3677           11.90m\n",
      "         6           1.3658           11.75m\n",
      "         7           1.3641           11.02m\n",
      "         8           1.3616           10.29m\n",
      "         9           1.3604            9.67m\n",
      "        10           1.3590            9.57m\n",
      "        20           1.3432            7.66m\n",
      "        30           1.3316            7.87m\n",
      "        40           1.3249            7.90m\n",
      "        50           1.3171            7.76m\n",
      "        60           1.3094            7.13m\n",
      "        70           1.3030            6.84m\n",
      "        80           1.2991            6.40m\n",
      "        90           1.2945            6.14m\n",
      "       100           1.2892            5.85m\n",
      "       200           1.2540            4.13m\n",
      "       300           1.2288            3.19m\n",
      "       400           1.2060            2.41m\n",
      "       500           1.1837            1.72m\n",
      "       600           1.1658            1.10m\n",
      "       700           1.1525           32.78s\n",
      "       800           1.1376            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3827            3.37m\n",
      "         2           1.3806            3.44m\n",
      "         3           1.3789            3.60m\n",
      "         4           1.3743            3.61m\n",
      "         5           1.3702            3.64m\n",
      "         6           1.3672            3.64m\n",
      "         7           1.3660            3.62m\n",
      "         8           1.3636            3.62m\n",
      "         9           1.3625            3.62m\n",
      "        10           1.3617            3.60m\n",
      "        20           1.3441            4.00m\n",
      "        30           1.3302            4.57m\n",
      "        40           1.3228            4.66m\n",
      "        50           1.3153            4.37m\n",
      "        60           1.3086            4.40m\n",
      "        70           1.3043            4.38m\n",
      "        80           1.2966            4.32m\n",
      "        90           1.2917            4.21m\n",
      "       100           1.2883            4.03m\n",
      "       200           1.2517            3.14m\n",
      "       300           1.2229            2.73m\n",
      "       400           1.2034            2.13m\n",
      "       500           1.1838            1.60m\n",
      "       600           1.1682            1.04m\n",
      "       700           1.1534           31.16s\n",
      "       800           1.1408            0.00s\n",
      "0.6770181818181819\n",
      "Cross-validated scores: [0.67521582 0.67655329 0.66577114]\n"
     ]
    }
   ],
   "source": [
    "scores_7 = cross_val_score(model_7, X_train,y_train, cv=3)   #3 fold validation\n",
    "print(accuracy_score(y_test,y_pred7))\n",
    "print (\"Cross-validated scores:\", scores_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 XGB Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>XGBClassifier with following parameters and capturing the performance metrics.\n",
    "    <ul><li>learning_rate =0.3</li>\n",
    "        <li>n_estimators=50</li>\n",
    "    <li>objective= 'binary:logistic'</li></ul></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /anaconda3/lib/python3.6/site-packages (0.81)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (from xgboost) (1.15.4)\n",
      "Requirement already satisfied: scipy in /anaconda3/lib/python3.6/site-packages (from xgboost) (1.1.0)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "0.6086909090909091\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.62      0.61     41073\n",
      "          4       0.61      0.60      0.61     41427\n",
      "\n",
      "avg / total       0.61      0.61      0.61     82500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "model_8 = XGBClassifier( learning_rate =0.3, n_estimators=50,   #build in cross validation #prune: -ve, stop spilt #missing val\n",
    "                        gamma=0, subsample=0.8, colsample_bytree=0.8,       #learning:slow down tree grow #sub,common used\n",
    "                        objective= 'binary:logistic', scale_pos_weight=1)           #scale :high class imba \n",
    "model_8.fit(X_train, y_train)                                                               \n",
    "y_pred8 = model_8.predict(X_test)                                                          \n",
    "print(accuracy_score(y_test, y_pred8))\n",
    "print(classification_report(y_test, y_pred8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation of XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6086909090909091\n",
      "Cross-validated scores: [0.60785543 0.60534093 0.60188419]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n"
     ]
    }
   ],
   "source": [
    "scores_8 = cross_val_score(model_8, X_train,y_train, cv=3)   #3 fold validation\n",
    "print(accuracy_score(y_test,y_pred8))\n",
    "print (\"Cross-validated scores:\", scores_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.9 XGB Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>XGBClassifier with following parameters and capturing the performance metrics.\n",
    "    <ul><li>learning_rate =0.2</li>\n",
    "        <li>n_estimators=500</li>\n",
    "    <li>objective= 'binary:logistic'</li></ul></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6648242424242424\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.65      0.66     41073\n",
      "          4       0.66      0.68      0.67     41427\n",
      "\n",
      "avg / total       0.66      0.66      0.66     82500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n"
     ]
    }
   ],
   "source": [
    "model_9 =XGBClassifier(learning_rate =0.2,n_estimators=500,\n",
    "                        gamma=0,subsample=0.8,colsample_bytree=0.8,\n",
    "                        objective= 'binary:logistic',scale_pos_weight=1)  \n",
    "model_9.fit(X_train,y_train)                                                                \n",
    "y_pred9 = model_9.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred9))\n",
    "print(classification_report(y_test, y_pred9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation for XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6648242424242424\n",
      "Cross-validated scores: [0.66113837 0.66159798 0.65678004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n"
     ]
    }
   ],
   "source": [
    "scores_9 = cross_val_score(model_9, X_train,y_train, cv=3)   #3 fold validation\n",
    "print(accuracy_score(y_test,y_pred9))\n",
    "print (\"Cross-validated scores:\", scores_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.10 Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Running the Ensemble by combining gradient boosting and random forest with the following parameters and capturing the performance metrics.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3859            2.34m\n",
      "         2           1.3856            2.03m\n",
      "         3           1.3853            1.82m\n",
      "         4           1.3850            1.73m\n",
      "         5           1.3848            1.70m\n",
      "         6           1.3846            1.77m\n",
      "         7           1.3844            1.71m\n",
      "         8           1.3842            1.66m\n",
      "         9           1.3841            1.65m\n",
      "        10           1.3839            1.63m\n",
      "        20           1.3829            1.52m\n",
      "        30           1.3821            1.45m\n",
      "        40           1.3815            1.41m\n",
      "        50           1.3810            1.37m\n",
      "        60           1.3805            1.35m\n",
      "        70           1.3800            1.31m\n",
      "        80           1.3796            1.29m\n",
      "        90           1.3791            1.27m\n",
      "       100           1.3787            1.24m\n",
      "       200           1.3747            1.04m\n",
      "       300           1.3716           51.75s\n",
      "       400           1.3688           41.17s\n",
      "       500           1.3664           30.78s\n",
      "       600           1.3642           20.79s\n",
      "       700           1.3623           10.40s\n",
      "       800           1.3605            0.00s\n",
      "0.6191272727272727\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.67      0.64     41073\n",
      "          4       0.63      0.57      0.60     41427\n",
      "\n",
      "avg / total       0.62      0.62      0.62     82500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n"
     ]
    }
   ],
   "source": [
    "gradientboosting = GradientBoostingClassifier(n_estimators=800,\n",
    "                                        max_features='auto', max_depth=1,\n",
    "                                        random_state=1, verbose=1)\n",
    "forest= RandomForestClassifier(n_estimators=120, random_state=0)\n",
    "model_10=VotingClassifier(estimators=[('Gradient Boost', gradientboosting), ('Random Forest', forest)], \n",
    "                       voting='soft', weights=[2,1])                           #weight focus on better model\n",
    "model_10.fit(X_train,y_train)                                                   #vote predicts the class label based on the argmax \n",
    "y_pred10 = model_10.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred10))\n",
    "print(classification_report(y_test, y_pred10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation for Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3859            1.38m\n",
      "         2           1.3856            1.39m\n",
      "         3           1.3853            1.57m\n",
      "         4           1.3851            1.61m\n",
      "         5           1.3848            1.44m\n",
      "         6           1.3846            1.42m\n",
      "         7           1.3844            1.40m\n",
      "         8           1.3843            1.41m\n",
      "         9           1.3841            1.40m\n",
      "        10           1.3840            1.42m\n",
      "        20           1.3830            1.30m\n",
      "        30           1.3822            1.18m\n",
      "        40           1.3817            1.06m\n",
      "        50           1.3811           59.56s\n",
      "        60           1.3806           56.71s\n",
      "        70           1.3802           54.24s\n",
      "        80           1.3797           52.42s\n",
      "        90           1.3792           50.65s\n",
      "       100           1.3788           49.19s\n",
      "       200           1.3748           39.09s\n",
      "       300           1.3715           31.62s\n",
      "       400           1.3688           25.05s\n",
      "       500           1.3664           19.57s\n",
      "       600           1.3642           12.90s\n",
      "       700           1.3622            6.38s\n",
      "       800           1.3604            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3859           50.63s\n",
      "         2           1.3856           49.60s\n",
      "         3           1.3853           49.31s\n",
      "         4           1.3850           51.00s\n",
      "         5           1.3848           50.46s\n",
      "         6           1.3846           50.04s\n",
      "         7           1.3844           49.72s\n",
      "         8           1.3842           50.56s\n",
      "         9           1.3840           50.24s\n",
      "        10           1.3839           49.75s\n",
      "        20           1.3828           48.32s\n",
      "        30           1.3820           47.62s\n",
      "        40           1.3814           47.16s\n",
      "        50           1.3809           46.57s\n",
      "        60           1.3804           46.32s\n",
      "        70           1.3800           45.64s\n",
      "        80           1.3795           45.12s\n",
      "        90           1.3791           44.43s\n",
      "       100           1.3787           43.71s\n",
      "       200           1.3749           37.63s\n",
      "       300           1.3718           31.48s\n",
      "       400           1.3691           25.19s\n",
      "       500           1.3667           18.83s\n",
      "       600           1.3645           12.52s\n",
      "       700           1.3626            6.23s\n",
      "       800           1.3608            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3859           48.20s\n",
      "         2           1.3855           48.54s\n",
      "         3           1.3852           49.90s\n",
      "         4           1.3850           51.90s\n",
      "         5           1.3847           51.57s\n",
      "         6           1.3845           50.84s\n",
      "         7           1.3843           50.45s\n",
      "         8           1.3842           51.10s\n",
      "         9           1.3840           51.56s\n",
      "        10           1.3838           51.22s\n",
      "        20           1.3827           49.86s\n",
      "        30           1.3819           49.11s\n",
      "        40           1.3813           48.53s\n",
      "        50           1.3807           48.09s\n",
      "        60           1.3802           47.78s\n",
      "        70           1.3797           47.13s\n",
      "        80           1.3792           46.69s\n",
      "        90           1.3787           46.06s\n",
      "       100           1.3783           45.42s\n",
      "       200           1.3743           38.93s\n",
      "       300           1.3710           32.49s\n",
      "       400           1.3682           25.99s\n",
      "       500           1.3657           19.46s\n",
      "       600           1.3634           12.94s\n",
      "       700           1.3614            6.47s\n",
      "       800           1.3595            0.00s\n",
      "0.6191272727272727\n",
      "Cross-validated scores: [0.61810008 0.61839772 0.61438576]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  unique_values = np.unique(values)\n"
     ]
    }
   ],
   "source": [
    "scores_10 = cross_val_score(model_10, X_train,y_train, cv=3)   #3 fold validation\n",
    "print(accuracy_score(y_test,y_pred10))\n",
    "print (\"Cross-validated scores:\", scores_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.11 AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>AdaBoost with DecisionTreeClassifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6215757575757576\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.61      0.62     41073\n",
      "          4       0.62      0.63      0.63     41427\n",
      "\n",
      "avg / total       0.62      0.62      0.62     82500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier() \n",
    "model_11= AdaBoostClassifier(base_estimator=dt, learning_rate=0.8, n_estimators=30)    #estimate: It controls the number of weak learners.\n",
    "model_11.fit(X_train,y_train)                                                   \n",
    "y_pred11 = model_11.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred11))\n",
    "print(classification_report(y_test, y_pred11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6215757575757576\n",
      "Cross-validated scores: [0.61621951 0.60575287 0.61624846]\n"
     ]
    }
   ],
   "source": [
    "scores_11 = cross_val_score(model_11, X_train,y_train, cv=3)   #3 fold validation\n",
    "print(accuracy_score(y_test,y_pred11))\n",
    "print (\"Cross-validated scores:\", scores_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing different classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the metrics of the different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>BernoulliNB</th>\n",
       "      <th>RF(est:50)</th>\n",
       "      <th>RF(est:500)</th>\n",
       "      <th>Ensemble</th>\n",
       "      <th>XGB(n:50)</th>\n",
       "      <th>DT</th>\n",
       "      <th>XGB(n:500)</th>\n",
       "      <th>GB(n:400)</th>\n",
       "      <th>GB(n:800)</th>\n",
       "      <th>AdaBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>49.773333</td>\n",
       "      <td>51.390303</td>\n",
       "      <td>53.390303</td>\n",
       "      <td>53.876364</td>\n",
       "      <td>57.589091</td>\n",
       "      <td>60.869091</td>\n",
       "      <td>59.078788</td>\n",
       "      <td>66.482424</td>\n",
       "      <td>61.549091</td>\n",
       "      <td>67.701818</td>\n",
       "      <td>62.157576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>33.258126</td>\n",
       "      <td>48.635167</td>\n",
       "      <td>53.345975</td>\n",
       "      <td>53.790609</td>\n",
       "      <td>57.396910</td>\n",
       "      <td>60.865831</td>\n",
       "      <td>59.067795</td>\n",
       "      <td>66.471103</td>\n",
       "      <td>61.549015</td>\n",
       "      <td>67.685639</td>\n",
       "      <td>62.151768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>49.773333</td>\n",
       "      <td>51.390303</td>\n",
       "      <td>53.390303</td>\n",
       "      <td>53.876364</td>\n",
       "      <td>57.589091</td>\n",
       "      <td>60.869091</td>\n",
       "      <td>59.078788</td>\n",
       "      <td>66.482424</td>\n",
       "      <td>61.549091</td>\n",
       "      <td>67.701818</td>\n",
       "      <td>62.157576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>49.773333</td>\n",
       "      <td>51.390303</td>\n",
       "      <td>53.390303</td>\n",
       "      <td>53.876364</td>\n",
       "      <td>57.589091</td>\n",
       "      <td>60.869091</td>\n",
       "      <td>59.078788</td>\n",
       "      <td>66.482424</td>\n",
       "      <td>61.549091</td>\n",
       "      <td>67.701818</td>\n",
       "      <td>62.157576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           GaussianNB  BernoulliNB  RF(est:50)  RF(est:500)   Ensemble  \\\n",
       "Accuracy    49.773333    51.390303   53.390303    53.876364  57.589091   \n",
       "F1_score    33.258126    48.635167   53.345975    53.790609  57.396910   \n",
       "Recall      49.773333    51.390303   53.390303    53.876364  57.589091   \n",
       "Precision   49.773333    51.390303   53.390303    53.876364  57.589091   \n",
       "\n",
       "           XGB(n:50)         DT  XGB(n:500)  GB(n:400)  GB(n:800)   AdaBoost  \n",
       "Accuracy   60.869091  59.078788   66.482424  61.549091  67.701818  62.157576  \n",
       "F1_score   60.865831  59.067795   66.471103  61.549015  67.685639  62.151768  \n",
       "Recall     60.869091  59.078788   66.482424  61.549091  67.701818  62.157576  \n",
       "Precision  60.869091  59.078788   66.482424  61.549091  67.701818  62.157576  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "Comparison = pd.DataFrame({'GaussianNB':[accuracy_score(y_test,y_pred4)*100,f1_score(y_test,y_pred4,average='macro')*100,recall_score(y_test, y_pred4,average='micro')*100,precision_score(y_test, y_pred4,average='micro')*100],\n",
    "                            'BernoulliNB':[accuracy_score(y_test,y_pred5)*100,f1_score(y_test,y_pred5,average='macro')*100,recall_score(y_test, y_pred5,average='micro')*100,precision_score(y_test, y_pred5,average='micro')*100],\n",
    "                            'RF(est:50)': [accuracy_score(y_test,y_pred2)*100,f1_score(y_test,y_pred2,average='macro')*100,recall_score(y_test, y_pred2,average='micro')*100,precision_score(y_test, y_pred2,average='micro')*100 ],\n",
    "                            'RF(est:500)':[accuracy_score(y_test,y_pred3)*100,f1_score(y_test,y_pred3,average='macro')*100,recall_score(y_test, y_pred3, average='micro')*100 ,precision_score(y_test, y_pred3,average='micro')*100],\n",
    "                            'Ensemble':[accuracy_score(y_test,y_pred10)*100,f1_score(y_test,y_pred10,average='macro')*100,recall_score(y_test, y_pred10,average='micro')*100,precision_score(y_test, y_pred10,average='micro')*100],\n",
    "                            'XGB(n:50)':[accuracy_score(y_test,y_pred8)*100,f1_score(y_test,y_pred8,average='macro')*100,recall_score(y_test, y_pred8,average='micro')*100,precision_score(y_test, y_pred8,average='micro')*100],\n",
    "                            'DT': [accuracy_score(y_test,y_pred1)*100,f1_score(y_test,y_pred1,average='macro')*100,recall_score(y_test, y_pred1,average='micro')*100,precision_score(y_test, y_pred1,average='micro')*100],\n",
    "                            'XGB(n:500)':[accuracy_score(y_test,y_pred9)*100,f1_score(y_test,y_pred9,average='macro')*100,recall_score(y_test, y_pred9,average='micro')*100,precision_score(y_test, y_pred9,average='micro')*100],\n",
    "                            'GB(n:400)':[accuracy_score(y_test,y_pred6)*100,f1_score(y_test,y_pred6,average='macro')*100,recall_score(y_test, y_pred6,average='micro')*100,precision_score(y_test, y_pred6,average='micro')*100],\n",
    "                            'GB(n:800)':[accuracy_score(y_test,y_pred7)*100,f1_score(y_test,y_pred7,average='macro')*100,recall_score(y_test, y_pred7,average='micro')*100,precision_score(y_test, y_pred7,average='micro')*100],\n",
    "                            'AdaBoost':[accuracy_score(y_test,y_pred11)*100,f1_score(y_test,y_pred11,average='macro')*100,recall_score(y_test, y_pred11,average='micro')*100,precision_score(y_test, y_pred11,average='micro')*100]})\n",
    "    \n",
    "    \n",
    "Comparison.rename(index={0:'Accuracy',1:'F1_score', 2: 'Recall',3:'Precision'}, inplace=True)\n",
    "Comparison.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1.  Tokenize and pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_most_common_words = 10000         \n",
    "max_len = 30\n",
    "tokenizer = Tokenizer(num_words=n_most_common_words)\n",
    "tokenizer.fit_on_texts(Dataset['tweet'].values)\n",
    "sequences = tokenizer.texts_to_sequences(Dataset['tweet'].values)\n",
    "X_Deep = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2. Categorized label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.0\n",
      "674998    0.0\n",
      "674999    0.0\n",
      "675000    0.0\n",
      "675001    0.0\n",
      "675002    0.0\n",
      "675003    0.0\n",
      "675004    0.0\n",
      "675005    0.0\n",
      "675006    0.0\n",
      "Name: LABEL, dtype: float64\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "Dataset.loc[Dataset['Sentiment'] == 0 , 'LABEL'] = 0                  #negative\n",
    "Dataset.loc[Dataset['Sentiment'] == 4, 'LABEL'] = 1                   #positive \n",
    "\n",
    "print(Dataset['LABEL'][:10])\n",
    "labels = to_categorical(Dataset['LABEL'], num_classes=2)\n",
    "print(labels[:])\n",
    "if 'Sentiment' in Dataset.keys():\n",
    "    Dataset.drop(['Sentiment'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.3. Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((167500, 30), (167500, 2), (82500, 30), (82500, 2))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_Deep , labels, test_size=0.33, random_state=42)\n",
    "print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.1.  1-Layer LSTM - with 15 Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Running 1 layer LSTM with the following parameters and capturing the performance metrics\n",
    "    <ul><li>LSTM = 128</li>\n",
    "        <li>activation = softmax</li>\n",
    "    <li>loss = binary</li></ul></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 30, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_9 (Spatial (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,411,842\n",
      "Trainable params: 1,411,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 134000 samples, validate on 33500 samples\n",
      "Epoch 1/15\n",
      "134000/134000 [==============================] - 214s 2ms/step - loss: 0.5459 - acc: 0.7151 - val_loss: 0.5183 - val_acc: 0.7404\n",
      "Epoch 2/15\n",
      "134000/134000 [==============================] - 192s 1ms/step - loss: 0.5049 - acc: 0.7481 - val_loss: 0.5128 - val_acc: 0.7413\n",
      "Epoch 3/15\n",
      "134000/134000 [==============================] - 197s 1ms/step - loss: 0.4915 - acc: 0.7566 - val_loss: 0.5107 - val_acc: 0.7420\n",
      "Epoch 4/15\n",
      "134000/134000 [==============================] - 178s 1ms/step - loss: 0.4816 - acc: 0.7619 - val_loss: 0.5117 - val_acc: 0.7434\n",
      "Epoch 5/15\n",
      "134000/134000 [==============================] - 180s 1ms/step - loss: 0.4718 - acc: 0.7669 - val_loss: 0.5137 - val_acc: 0.7426\n",
      "Epoch 6/15\n",
      "134000/134000 [==============================] - 183s 1ms/step - loss: 0.4628 - acc: 0.7723 - val_loss: 0.5233 - val_acc: 0.7430\n",
      "Epoch 7/15\n",
      "134000/134000 [==============================] - 217s 2ms/step - loss: 0.4553 - acc: 0.7759 - val_loss: 0.5193 - val_acc: 0.7412\n",
      "Epoch 8/15\n",
      "134000/134000 [==============================] - 195s 1ms/step - loss: 0.4474 - acc: 0.7814 - val_loss: 0.5347 - val_acc: 0.7413\n",
      "Epoch 9/15\n",
      "134000/134000 [==============================] - 175s 1ms/step - loss: 0.4401 - acc: 0.7851 - val_loss: 0.5387 - val_acc: 0.7394\n",
      "Epoch 10/15\n",
      "134000/134000 [==============================] - 287s 2ms/step - loss: 0.4349 - acc: 0.7876 - val_loss: 0.5480 - val_acc: 0.7397\n",
      "Epoch 11/15\n",
      "134000/134000 [==============================] - 200s 1ms/step - loss: 0.4279 - acc: 0.7909 - val_loss: 0.5592 - val_acc: 0.7368\n",
      "Epoch 12/15\n",
      "134000/134000 [==============================] - 168s 1ms/step - loss: 0.4225 - acc: 0.7951 - val_loss: 0.5629 - val_acc: 0.7365\n",
      "Epoch 13/15\n",
      "134000/134000 [==============================] - 166s 1ms/step - loss: 0.4166 - acc: 0.7972 - val_loss: 0.5753 - val_acc: 0.7345\n",
      "Epoch 14/15\n",
      "134000/134000 [==============================] - 188s 1ms/step - loss: 0.4107 - acc: 0.7997 - val_loss: 0.5725 - val_acc: 0.7338\n",
      "Epoch 15/15\n",
      "134000/134000 [==============================] - 163s 1ms/step - loss: 0.4058 - acc: 0.8043 - val_loss: 0.5781 - val_acc: 0.7350\n"
     ]
    }
   ],
   "source": [
    "epochs = 15                                                                       #run 15 iterations\n",
    "emb_dim = 128                                                                     #set embbeding dimension as 128\n",
    "batch_size = 100                                                                  # Run 100 at a time. higher, training will be faster\n",
    "model0 = Sequential()\n",
    "model0.add(Embedding(n_most_common_words,emb_dim, input_length=X_Deep.shape[1]))  #use input length, and common words as embbeding layer\n",
    "model0.add(SpatialDropout1D(0.5))                                                 #Remove 1D from the neurons \n",
    "model0.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5))                         #128 as hidden layer # close 50% for each neural layer  \n",
    "model0.add(Dense(2, activation='softmax'))                                        #return output value as sum 1\n",
    "model0.compile(optimizer=tf.train.AdamOptimizer(),loss='binary_crossentropy', metrics=['acc']) #compile model by optimize it with adam\n",
    "print(model0.summary())                                                                             # show the summary of model build\n",
    "history0 = model0.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)  #add validation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.2.  2-Layer LSTM - with 15 Epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Running 2 layers LSTM with the following parameters and capturing the performance metrics\n",
    "    <ul><li>LSTM = 100</li>\n",
    "         <li>LSTM = 50</li>\n",
    "        <li>activation = softmax</li>\n",
    "    <li>loss = binary</li></ul></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 30, 256)           2560000   \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 30, 100)           142800    \n",
      "_________________________________________________________________\n",
      "lstm_40 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,733,102\n",
      "Trainable params: 2,733,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 134000 samples, validate on 33500 samples\n",
      "Epoch 1/15\n",
      "134000/134000 [==============================] - 295s 2ms/step - loss: 0.5438 - acc: 0.7161 - val_loss: 0.5155 - val_acc: 0.7423\n",
      "Epoch 2/15\n",
      "134000/134000 [==============================] - 335s 3ms/step - loss: 0.4939 - acc: 0.7539 - val_loss: 0.5146 - val_acc: 0.7412\n",
      "Epoch 3/15\n",
      "134000/134000 [==============================] - 297s 2ms/step - loss: 0.4747 - acc: 0.7651 - val_loss: 0.5176 - val_acc: 0.7390\n",
      "Epoch 4/15\n",
      "134000/134000 [==============================] - 306s 2ms/step - loss: 0.4578 - acc: 0.7739 - val_loss: 0.5312 - val_acc: 0.7396\n",
      "Epoch 5/15\n",
      "134000/134000 [==============================] - 292s 2ms/step - loss: 0.4423 - acc: 0.7814 - val_loss: 0.5586 - val_acc: 0.7359\n",
      "Epoch 6/15\n",
      "134000/134000 [==============================] - 303s 2ms/step - loss: 0.4296 - acc: 0.7875 - val_loss: 0.5535 - val_acc: 0.7345\n",
      "Epoch 7/15\n",
      "134000/134000 [==============================] - 288s 2ms/step - loss: 0.4183 - acc: 0.7926 - val_loss: 0.5812 - val_acc: 0.7307\n",
      "Epoch 8/15\n",
      "134000/134000 [==============================] - 308s 2ms/step - loss: 0.4083 - acc: 0.7995 - val_loss: 0.6122 - val_acc: 0.7320\n",
      "Epoch 9/15\n",
      "134000/134000 [==============================] - 282s 2ms/step - loss: 0.3978 - acc: 0.8035 - val_loss: 0.6309 - val_acc: 0.7309\n",
      "Epoch 10/15\n",
      "134000/134000 [==============================] - 324s 2ms/step - loss: 0.3884 - acc: 0.8077 - val_loss: 0.6434 - val_acc: 0.7278\n",
      "Epoch 11/15\n",
      "134000/134000 [==============================] - 241s 2ms/step - loss: 0.3798 - acc: 0.8123 - val_loss: 0.6698 - val_acc: 0.7271\n",
      "Epoch 12/15\n",
      "134000/134000 [==============================] - 228s 2ms/step - loss: 0.3707 - acc: 0.8178 - val_loss: 0.6836 - val_acc: 0.7257\n",
      "Epoch 13/15\n",
      "134000/134000 [==============================] - 230s 2ms/step - loss: 0.3612 - acc: 0.8223 - val_loss: 0.7284 - val_acc: 0.7254\n",
      "Epoch 14/15\n",
      "134000/134000 [==============================] - 238s 2ms/step - loss: 0.3541 - acc: 0.8262 - val_loss: 0.7457 - val_acc: 0.7225\n",
      "Epoch 15/15\n",
      "134000/134000 [==============================] - 237s 2ms/step - loss: 0.3457 - acc: 0.8316 - val_loss: 0.7702 - val_acc: 0.7230\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "emb_dim = 256\n",
    "batch_size = 200\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(n_most_common_words,emb_dim ,input_length=X_Deep.shape[1]))\n",
    "model2.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))\n",
    "model2.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))                                        \n",
    "model2.add(Dense(2, activation='softmax'))\n",
    "model2.compile(optimizer=tf.train.AdamOptimizer(),loss='binary_crossentropy', metrics=['acc']) \n",
    "print(model2.summary())                                                                          \n",
    "history2 = model2.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  10.1.3. CNN + 2LSTM with 15 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Running CNN + 2 layers LSTM with the following parameters and capturing the performance metrics\n",
    "    <ul><li>LSTM = 300</li>\n",
    "         <li>LSTM = 150</li>\n",
    "        <li>activation = softmax, relu</li>\n",
    "        <li>pooling size = 5</li>\n",
    "    <li>loss = binary</li></ul></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_27 (Embedding)     (None, 30, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 6, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 6, 300)            514800    \n",
      "_________________________________________________________________\n",
      "lstm_42 (LSTM)               (None, 150)               270600    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2)                 302       \n",
      "=================================================================\n",
      "Total params: 2,114,982\n",
      "Trainable params: 2,114,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 134000 samples, validate on 33500 samples\n",
      "Epoch 1/15\n",
      "134000/134000 [==============================] - 180s 1ms/step - loss: 0.5773 - acc: 0.6807 - val_loss: 0.5172 - val_acc: 0.7392\n",
      "Epoch 2/15\n",
      "134000/134000 [==============================] - 165s 1ms/step - loss: 0.4945 - acc: 0.7571 - val_loss: 0.5155 - val_acc: 0.7409\n",
      "Epoch 3/15\n",
      "134000/134000 [==============================] - 167s 1ms/step - loss: 0.4712 - acc: 0.7731 - val_loss: 0.5230 - val_acc: 0.7405\n",
      "Epoch 4/15\n",
      "134000/134000 [==============================] - 163s 1ms/step - loss: 0.4488 - acc: 0.7864 - val_loss: 0.5320 - val_acc: 0.7391\n",
      "Epoch 5/15\n",
      "134000/134000 [==============================] - 160s 1ms/step - loss: 0.4234 - acc: 0.8017 - val_loss: 0.5369 - val_acc: 0.7350\n",
      "Epoch 6/15\n",
      "134000/134000 [==============================] - 160s 1ms/step - loss: 0.4016 - acc: 0.8131 - val_loss: 0.5635 - val_acc: 0.7327\n",
      "Epoch 7/15\n",
      "134000/134000 [==============================] - 152s 1ms/step - loss: 0.3774 - acc: 0.8261 - val_loss: 0.5843 - val_acc: 0.7321\n",
      "Epoch 8/15\n",
      "134000/134000 [==============================] - 167s 1ms/step - loss: 0.3561 - acc: 0.8361 - val_loss: 0.6060 - val_acc: 0.7293\n",
      "Epoch 9/15\n",
      "134000/134000 [==============================] - 180s 1ms/step - loss: 0.3342 - acc: 0.8479 - val_loss: 0.6689 - val_acc: 0.7276\n",
      "Epoch 10/15\n",
      "134000/134000 [==============================] - 179s 1ms/step - loss: 0.3093 - acc: 0.8610 - val_loss: 0.6981 - val_acc: 0.7240\n",
      "Epoch 11/15\n",
      "134000/134000 [==============================] - 159s 1ms/step - loss: 0.2852 - acc: 0.8716 - val_loss: 0.7620 - val_acc: 0.7230\n",
      "Epoch 12/15\n",
      "134000/134000 [==============================] - 172s 1ms/step - loss: 0.2674 - acc: 0.8801 - val_loss: 0.7887 - val_acc: 0.7223\n",
      "Epoch 13/15\n",
      "134000/134000 [==============================] - 163s 1ms/step - loss: 0.2520 - acc: 0.8868 - val_loss: 0.8188 - val_acc: 0.7201\n",
      "Epoch 14/15\n",
      "134000/134000 [==============================] - 160s 1ms/step - loss: 0.2409 - acc: 0.8913 - val_loss: 0.8529 - val_acc: 0.7180\n",
      "Epoch 15/15\n",
      "134000/134000 [==============================] - 162s 1ms/step - loss: 0.2303 - acc: 0.8962 - val_loss: 0.9121 - val_acc: 0.7168\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "emb_dim = 128\n",
    "batch_size = 1000\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(n_most_common_words,emb_dim, input_length=X_Deep.shape[1]))   #CNN- do all the heavy computation\n",
    "model1.add(Conv1D(128, 3, padding='same'))               #Conv1D used text, 2D for image , same, input length= output\n",
    "model1.add(Activation('relu'))                            #128 filters, 3 windows , relu return 0 value if -ve exist\n",
    "model1.add(MaxPooling1D(pool_size=5))                   #reduce size, to reduce the amount of parameters/computation in the network.             \n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(LSTM(300, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))\n",
    "model1.add(LSTM(150, dropout=0.5, recurrent_dropout=0.7))\n",
    "model1.add(Dense(2, activation='softmax'))\n",
    "model1.compile(optimizer=tf.train.AdamOptimizer(),loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model1.summary())                                                                           \n",
    "history1 = model1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Save Model in hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.6/site-packages (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.7 in /anaconda3/lib/python3.6/site-packages (from h5py) (1.15.4)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from h5py) (1.11.0)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py\n",
    "from keras.models import model_from_json                     #import the keras model \n",
    "model1_json = model1.to_json()                                  #save it in json file\n",
    "with open(\"/Users/yeezhianliew/Desktop/model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model1_json)\n",
    "model1.save_weights(\"/Users/yeezhianliew/Desktop/model1.h1\")\n",
    "print(\"Saved model to disk\")                                  #save the model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
